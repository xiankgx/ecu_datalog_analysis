{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion/mapping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pressure_kpa_abs_to_kpa(val):\n",
    "    return val - 101.3\n",
    "\n",
    "def convert_afr_lambda_to_afr(val):\n",
    "    return val * 14.7\n",
    "\n",
    "def map_afr_diff(raw_val):\n",
    "    return convert_afr_lambda_to_afr(raw_val / 1000)\n",
    "\n",
    "def map_load(raw_val):\n",
    "    return convert_pressure_kpa_abs_to_kpa(raw_val / 10)\n",
    "\n",
    "def convert_temp_kelvin_to_celsius(val):\n",
    "    return val - 273.15\n",
    "\n",
    "def map_temp(raw_val):\n",
    "    return convert_temp_kelvin_to_celsius(raw_val / 10)\n",
    "\n",
    "def map_base_ignition(raw_val):\n",
    "    return raw_val / 10 - 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_file_path = 'datalogs/20180713-mimos2home.csv'\n",
    "    \n",
    "def pandas_dataframe_from_datalog_csv(log_file_path):\n",
    "    \"\"\"\n",
    "    Read from datalog csv file and return a pandas dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_line=0\n",
    "    content=\"\"\n",
    "    with open(log_file_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "    #        print(line_num, line)\n",
    "            \n",
    "            # Found log start line\n",
    "            if line.find(\"Log : \") != -1:\n",
    "    #            print(line.find(\"Log : \"))\n",
    "    #            print(\"break\")\n",
    "                start_line = line_num + 1\n",
    "                print(start_line)\n",
    "                break\n",
    "            \n",
    "            content = content + line\n",
    "            \n",
    "    channels = []\n",
    "    channel_types = set()\n",
    "    \n",
    "    headers = [\"Time\"]\n",
    "    pattern = r\"Channel : (?P<channel>.+)\\n*Type : (?P<type>.+)\\n*DisplayMaxMin : (?P<max>.+),(?P<min>.+)\\n*\"\n",
    "    for match in re.finditer(pattern, content):\n",
    "        channel = match.group(\"channel\")\n",
    "        headers.append(channel)\n",
    "        \n",
    "        channel_type = match.group(\"type\")\n",
    "        channel_types.add(channel_type)\n",
    "        \n",
    "        val_max = match.group(\"max\")\n",
    "        val_min = match.group(\"min\")\n",
    "        \n",
    "        channels.append({\n",
    "                \"Name\": channel,\n",
    "                \"Type\": channel_type,\n",
    "                \"Min\": val_min,\n",
    "                \"Max\": val_max\n",
    "                })\n",
    "    \n",
    "        \n",
    "    print(\"# channels:\", len(headers) - 1)\n",
    "    print(\"# channel types:\", len(channel_types))      \n",
    "    \n",
    "    # Read csv log file\n",
    "    \n",
    "    df=pd.read_csv(log_file_path, skiprows=start_line, header=None)\n",
    "#    df.head()\n",
    "    \n",
    "    # Rename headers\n",
    "    df.columns = headers\n",
    "\n",
    "    # Parse and set time as index\n",
    "    df.index = pd.to_datetime(df.Time)\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "    \n",
    "    for channel in channels:\n",
    "        channel_name = channel[\"Name\"]\n",
    "        channel_type = channel[\"Type\"]\n",
    "        val_min = channel[\"Min\"]\n",
    "        val_max = channel[\"Max\"]\n",
    "    \n",
    "        if channel_type == \"Percentage\":\n",
    "            df[channel_name] = df[channel_name] / 10\n",
    "        elif channel_type == \"Temperature\":\n",
    "            df[channel_name] = df[channel_name].apply(map_temp)\n",
    "        elif channel_type == \"Pressure\":\n",
    "            df[channel_name] = df[channel_name].apply(map_load)\n",
    "        elif channel_type == \"AFR\":\n",
    "            df[channel_name] = df[channel_name].apply(map_afr_diff)\n",
    "        elif channel_type == \"BatteryVoltage\":\n",
    "            df[channel_name] = df[channel_name] / 1000\n",
    "        elif channel_type == \"AngleIgnSprt2K\":\n",
    "            df[channel_name] = df[channel_name].apply(map_base_ignition)\n",
    "        elif channel_type == \"Angle\":\n",
    "            df[channel_name] = df[channel_name] / 10\n",
    "        elif channel_type in [\"Time_s\", \"EngineSpeed\", \"Raw\"]:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Unknown channel type:\", channel_name, \":\", channel_type)\n",
    "\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    #df.dropna(axis=0, how='any', inplace=True\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data log paths and load data logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n",
      "84\n",
      "# channels: 26\n",
      "# channel types: 14\n",
      "Unknown channel type: FuelCoolantTempCorrection : Percentage1For1\n",
      "Unknown channel type: IgnitionCoolantTempCorrection : AngleOffset10deg\n",
      "Unknown channel type: TransientThrottleEnrichSensitivity : Time_us\n",
      "Unknown channel type: TransientThrottleEnrichDecayRate : msPerEngCyl\n"
     ]
    }
   ],
   "source": [
    "LOG_FILE_PATHS = [\n",
    "        'datalogs/20180716-home2mimos.csv',\n",
    "        'datalogs/20180716-mimos2home.csv',\n",
    "        \n",
    "        'datalogs/20180717-home2mimos.csv',\n",
    "        'datalogs/20180717-mimos2home.csv',\n",
    "        \n",
    "        'datalogs/20180718-home2mimos.csv',\n",
    "        'datalogs/20180718-mimos2home.csv',\n",
    "        \n",
    "        'datalogs/20180719-home2mimos.csv',\n",
    "        'datalogs/20180719-mimos2home.csv',\n",
    "        \n",
    "        'datalogs/20180720-home2mimos.csv',\n",
    "        'datalogs/20180720-mimos2home.csv',\n",
    "        ]\n",
    "        \n",
    "dfs=[]\n",
    "for log_file_path in LOG_FILE_PATHS:\n",
    "    df=pandas_dataframe_from_datalog_csv(log_file_path)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)\n",
    "# Concat all the dataframes from different datalogs\n",
    "#df=pd.concat([df for i, df in enumerate(dfs) if i < 7], axis=0)\n",
    "df=pd.concat([df for i, df in enumerate(dfs)], axis=0)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define features and targets/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=['RPM', 'Load', 'TargetAFR', 'AFRDifference', 'AirTemp', 'CoolantTemp']\n",
    "features=['RPM', 'Load', 'AFRDifference']\n",
    "targets=['BaseFuel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters and training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "NUM_HIDDEN_LAYERS = 5\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.3\n",
    "NUM_EPOCHS = 200\n",
    "HIDDEN_LAYER_NEURONS = 100\n",
    "DROPOUT_RATE=0.2\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'mse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181896 samples, validate on 77956 samples\n",
      "Epoch 1/200\n",
      "181896/181896 [==============================] - 27s 147us/step - loss: 0.1244 - val_loss: 0.0101\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01006, saving model to models/model2-01-0.0101.hdf5\n",
      "Epoch 2/200\n",
      "181896/181896 [==============================] - 23s 125us/step - loss: 0.0385 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01006 to 0.00878, saving model to models/model2-02-0.0088.hdf5\n",
      "Epoch 3/200\n",
      "181896/181896 [==============================] - 24s 131us/step - loss: 0.0320 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00878 to 0.00852, saving model to models/model2-03-0.0085.hdf5\n",
      "Epoch 4/200\n",
      "181896/181896 [==============================] - 23s 124us/step - loss: 0.0283 - val_loss: 0.0061\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00852 to 0.00613, saving model to models/model2-04-0.0061.hdf5\n",
      "Epoch 5/200\n",
      "181896/181896 [==============================] - 23s 126us/step - loss: 0.0265 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00613 to 0.00281, saving model to models/model2-05-0.0028.hdf5\n",
      "Epoch 6/200\n",
      "181896/181896 [==============================] - 23s 124us/step - loss: 0.0247 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00281\n",
      "Epoch 7/200\n",
      "181896/181896 [==============================] - 24s 129us/step - loss: 0.0225 - val_loss: 0.0045\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00281\n",
      "Epoch 8/200\n",
      "181896/181896 [==============================] - 23s 127us/step - loss: 0.0230 - val_loss: 0.0045\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00281\n",
      "Epoch 9/200\n",
      "181896/181896 [==============================] - 23s 124us/step - loss: 0.0216 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00281 to 0.00246, saving model to models/model2-09-0.0025.hdf5\n",
      "Epoch 10/200\n",
      "181896/181896 [==============================] - 23s 124us/step - loss: 0.0207 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00246 to 0.00228, saving model to models/model2-10-0.0023.hdf5\n",
      "Epoch 11/200\n",
      "181896/181896 [==============================] - 23s 127us/step - loss: 0.0196 - val_loss: 0.0062\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00228\n",
      "Epoch 12/200\n",
      "181896/181896 [==============================] - 22s 121us/step - loss: 0.0194 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00228 to 0.00185, saving model to models/model2-12-0.0019.hdf5\n",
      "Epoch 13/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0197 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00185\n",
      "Epoch 14/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0185 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00185\n",
      "Epoch 15/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0186 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00185\n",
      "Epoch 16/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0189 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00185\n",
      "Epoch 17/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0185 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00185\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/200\n",
      "181896/181896 [==============================] - 22s 121us/step - loss: 0.0172 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00185\n",
      "Epoch 19/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0173 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00185\n",
      "Epoch 20/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00185\n",
      "Epoch 21/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00185 to 0.00146, saving model to models/model2-21-0.0015.hdf5\n",
      "Epoch 22/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0170 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00146\n",
      "Epoch 23/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0164 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00146\n",
      "Epoch 24/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0173 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00146\n",
      "Epoch 25/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0166 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00146\n",
      "Epoch 26/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 27/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00146\n",
      "Epoch 28/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00146\n",
      "Epoch 29/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00146\n",
      "Epoch 30/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0166 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00146\n",
      "Epoch 31/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 32/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00146\n",
      "Epoch 33/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00146\n",
      "Epoch 34/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00146\n",
      "Epoch 35/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00146\n",
      "Epoch 36/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0168 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 37/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00146\n",
      "Epoch 38/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00146\n",
      "Epoch 39/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00146\n",
      "Epoch 40/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0159 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00146\n",
      "Epoch 41/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 42/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00146\n",
      "Epoch 43/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00146\n",
      "Epoch 44/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0174 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00146\n",
      "Epoch 45/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00146\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 47/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0174 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00146\n",
      "Epoch 48/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0165 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00146\n",
      "Epoch 49/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00146\n",
      "Epoch 50/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0163 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00146\n",
      "Epoch 51/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 52/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0158 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00146\n",
      "Epoch 53/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00146\n",
      "Epoch 54/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00146\n",
      "Epoch 55/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0160 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00146\n",
      "Epoch 56/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 57/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00146\n",
      "Epoch 58/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00146\n",
      "Epoch 59/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00146\n",
      "Epoch 60/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0167 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00146\n",
      "Epoch 61/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 62/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00146\n",
      "Epoch 63/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00146\n",
      "Epoch 64/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00146\n",
      "Epoch 65/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00146\n",
      "Epoch 66/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 67/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00146\n",
      "Epoch 68/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00146\n",
      "Epoch 69/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00146\n",
      "Epoch 70/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0163 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00146\n",
      "Epoch 71/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 72/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00146\n",
      "Epoch 73/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00146\n",
      "Epoch 74/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00146\n",
      "Epoch 75/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00146\n",
      "Epoch 76/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 77/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0163 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00146\n",
      "Epoch 78/200\n",
      "181896/181896 [==============================] - 22s 124us/step - loss: 0.0173 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00146\n",
      "Epoch 79/200\n",
      "181896/181896 [==============================] - 23s 127us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00146\n",
      "Epoch 80/200\n",
      "181896/181896 [==============================] - 23s 125us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00146\n",
      "Epoch 81/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 82/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00146\n",
      "Epoch 83/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0164 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00146\n",
      "Epoch 84/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0162 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00146\n",
      "Epoch 85/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00146\n",
      "Epoch 86/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 87/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0172 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00146\n",
      "Epoch 88/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00146\n",
      "Epoch 89/200\n",
      "181896/181896 [==============================] - 21s 116us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00146\n",
      "Epoch 90/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0162 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00146\n",
      "Epoch 91/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0164 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181896/181896 [==============================] - 21s 116us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00146\n",
      "Epoch 93/200\n",
      "181896/181896 [==============================] - 21s 115us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00146\n",
      "Epoch 94/200\n",
      "181896/181896 [==============================] - 21s 116us/step - loss: 0.0172 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00146\n",
      "Epoch 95/200\n",
      "181896/181896 [==============================] - 21s 115us/step - loss: 0.0164 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00146\n",
      "Epoch 96/200\n",
      "181896/181896 [==============================] - 21s 115us/step - loss: 0.0163 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 97/200\n",
      "181896/181896 [==============================] - 21s 116us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00146\n",
      "Epoch 98/200\n",
      "181896/181896 [==============================] - 21s 115us/step - loss: 0.0165 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00146\n",
      "Epoch 99/200\n",
      "181896/181896 [==============================] - 21s 115us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00146\n",
      "Epoch 100/200\n",
      "181896/181896 [==============================] - 21s 116us/step - loss: 0.0166 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00146\n",
      "Epoch 101/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 102/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0165 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00146\n",
      "Epoch 103/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00146\n",
      "Epoch 104/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00146\n",
      "Epoch 105/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00146\n",
      "Epoch 106/200\n",
      "181896/181896 [==============================] - 21s 117us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 107/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0171 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00146\n",
      "Epoch 108/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0164 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00146\n",
      "Epoch 109/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00146\n",
      "Epoch 110/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0173 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00146\n",
      "Epoch 111/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0174 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 112/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00146\n",
      "Epoch 113/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0174 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00146\n",
      "Epoch 114/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00146\n",
      "Epoch 115/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00146\n",
      "Epoch 116/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "Epoch 117/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00146\n",
      "Epoch 118/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00146\n",
      "Epoch 119/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00146\n",
      "Epoch 120/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00146\n",
      "Epoch 121/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0169 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "Epoch 122/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00146\n",
      "Epoch 123/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00146\n",
      "Epoch 124/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00146\n",
      "Epoch 125/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00146\n",
      "Epoch 126/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0165 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
      "Epoch 127/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00146\n",
      "Epoch 128/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00146\n",
      "Epoch 129/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00146\n",
      "Epoch 130/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00146\n",
      "Epoch 131/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
      "Epoch 132/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00146\n",
      "Epoch 133/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00146\n",
      "Epoch 134/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00146\n",
      "Epoch 135/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00146\n",
      "Epoch 136/200\n",
      "181896/181896 [==============================] - 22s 121us/step - loss: 0.0169 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
      "Epoch 137/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00146\n",
      "Epoch 138/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0173 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00146\n",
      "Epoch 139/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00146\n",
      "Epoch 140/200\n",
      "181896/181896 [==============================] - 23s 127us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00146\n",
      "Epoch 141/200\n",
      "181896/181896 [==============================] - 22s 122us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
      "Epoch 142/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00146\n",
      "Epoch 143/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00146\n",
      "Epoch 144/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00146\n",
      "Epoch 145/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00146\n",
      "Epoch 146/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 1.0000001235416985e-29.\n",
      "Epoch 147/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0175 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00146\n",
      "Epoch 148/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00146\n",
      "Epoch 149/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00146\n",
      "Epoch 150/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00146\n",
      "Epoch 151/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.0000001536343539e-30.\n",
      "Epoch 152/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00146\n",
      "Epoch 153/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00146\n",
      "Epoch 154/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00146\n",
      "Epoch 155/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0160 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00146\n",
      "Epoch 156/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 1.000000191250173e-31.\n",
      "Epoch 157/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00146\n",
      "Epoch 158/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00146\n",
      "Epoch 159/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00146\n",
      "Epoch 160/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00146\n",
      "Epoch 161/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 1.0000002147600601e-32.\n",
      "Epoch 162/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00146\n",
      "Epoch 163/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00146\n",
      "Epoch 164/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00146\n",
      "Epoch 165/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00146\n",
      "Epoch 166/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 1.0000002441474188e-33.\n",
      "Epoch 167/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00146\n",
      "Epoch 168/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0162 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00146\n",
      "Epoch 169/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00146\n",
      "Epoch 170/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00146\n",
      "Epoch 171/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1.0000002074132203e-34.\n",
      "Epoch 172/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0164 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00146\n",
      "Epoch 173/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00146\n",
      "Epoch 174/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00146\n",
      "Epoch 175/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00146\n",
      "Epoch 176/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 1.0000001614954722e-35.\n",
      "Epoch 177/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0170 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00146\n",
      "Epoch 178/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0172 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00146\n",
      "Epoch 179/200\n",
      "181896/181896 [==============================] - 22s 120us/step - loss: 0.0168 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00146\n",
      "Epoch 180/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0173 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00146\n",
      "Epoch 181/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 1.0000001614954723e-36.\n",
      "Epoch 182/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00146\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0162 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00146\n",
      "Epoch 184/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0170 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00146\n",
      "Epoch 185/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00146\n",
      "Epoch 186/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0174 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 1.0000001256222317e-37.\n",
      "Epoch 187/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0165 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00146\n",
      "Epoch 188/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00146\n",
      "Epoch 189/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00146\n",
      "Epoch 190/200\n",
      "181896/181896 [==============================] - 21s 118us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00146\n",
      "Epoch 191/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0164 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 1.0000001032014561e-38.\n",
      "Epoch 192/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0171 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00146\n",
      "Epoch 193/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0167 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00146\n",
      "Epoch 194/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00146\n",
      "Epoch 195/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0166 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00146\n",
      "Epoch 196/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0167 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00146\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 1.0000000751754869e-39.\n",
      "Epoch 197/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0168 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00146\n",
      "Epoch 198/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0169 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00146\n",
      "Epoch 199/200\n",
      "181896/181896 [==============================] - 22s 119us/step - loss: 0.0170 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00146\n",
      "Epoch 200/200\n",
      "181896/181896 [==============================] - 22s 118us/step - loss: 0.0168 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00146\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x=df[features].values\n",
    "y=df[targets].values\n",
    "\n",
    "scaler=StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "\n",
    "scaler2=StandardScaler()\n",
    "y=scaler2.fit_transform(y)\n",
    "\n",
    "# Define DNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(HIDDEN_LAYER_NEURONS, input_shape=(len(features),), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "    model.add(Dense(HIDDEN_LAYER_NEURONS, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "model.add(Dense(len(targets), activation='linear'))\n",
    "\n",
    "checkpointer=ModelCheckpoint(\"models/model2-{epoch:02d}-{val_loss:.4f}.hdf5\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5,\n",
    "                              verbose=1)\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=LOSS)\n",
    "\n",
    "hist=model.fit(x, y,\n",
    "               batch_size=BATCH_SIZE,\n",
    "               shuffle=True,\n",
    "               validation_split=VALIDATION_SPLIT,\n",
    "               epochs=NUM_EPOCHS,\n",
    "               callbacks=[checkpointer, reduce_lr,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_fuel(rpm, load, afrdiff=0, targetafr=14.7, airtemp=60, coolanttemp=80,\n",
    "                      model=model,\n",
    "                      xscaler=scaler,\n",
    "                      yscaler=scaler2):\n",
    "#    x = np.array([rpm, load, targetafr, afrdiff, airtemp, coolanttemp])\n",
    "    x = np.array([rpm, load, afrdiff])\n",
    "    x = x.reshape(1, -1)\n",
    "    x = xscaler.transform(x)\n",
    "    y_predict = model.predict(x)\n",
    "    y_predict = yscaler.inverse_transform(y_predict)\n",
    "    return y_predict.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fuel map RPMs and load points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_rpms=[10000, 8000, 7000, 6000, 5500, 5000, 4000, 3500, 3000, 2500, 2000, 1500, 1000, 500, 0]\n",
    "fuel_loads=[-100, -85, -80, -70, -60, -50, -40, -30, -20, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xian_\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-100</th>\n",
       "      <th>-85</th>\n",
       "      <th>-80</th>\n",
       "      <th>-70</th>\n",
       "      <th>-60</th>\n",
       "      <th>-50</th>\n",
       "      <th>-40</th>\n",
       "      <th>-30</th>\n",
       "      <th>-20</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>95.156166</td>\n",
       "      <td>97.072845</td>\n",
       "      <td>98.251839</td>\n",
       "      <td>101.292374</td>\n",
       "      <td>104.738571</td>\n",
       "      <td>107.212234</td>\n",
       "      <td>109.492973</td>\n",
       "      <td>112.391235</td>\n",
       "      <td>114.940308</td>\n",
       "      <td>120.652802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>80.992577</td>\n",
       "      <td>80.656555</td>\n",
       "      <td>81.296867</td>\n",
       "      <td>83.045113</td>\n",
       "      <td>85.806847</td>\n",
       "      <td>89.005676</td>\n",
       "      <td>91.346283</td>\n",
       "      <td>94.262543</td>\n",
       "      <td>96.579269</td>\n",
       "      <td>102.227219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>72.939873</td>\n",
       "      <td>73.574600</td>\n",
       "      <td>73.921783</td>\n",
       "      <td>74.942772</td>\n",
       "      <td>76.689171</td>\n",
       "      <td>79.570923</td>\n",
       "      <td>82.304031</td>\n",
       "      <td>85.109131</td>\n",
       "      <td>87.276886</td>\n",
       "      <td>92.838837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>56.404900</td>\n",
       "      <td>66.701172</td>\n",
       "      <td>66.736687</td>\n",
       "      <td>67.701447</td>\n",
       "      <td>68.986969</td>\n",
       "      <td>70.598473</td>\n",
       "      <td>73.476135</td>\n",
       "      <td>75.995560</td>\n",
       "      <td>77.774910</td>\n",
       "      <td>82.798866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>36.528988</td>\n",
       "      <td>63.695129</td>\n",
       "      <td>63.519688</td>\n",
       "      <td>64.266251</td>\n",
       "      <td>65.331619</td>\n",
       "      <td>66.565918</td>\n",
       "      <td>68.902870</td>\n",
       "      <td>71.466042</td>\n",
       "      <td>73.106491</td>\n",
       "      <td>77.596542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>29.380203</td>\n",
       "      <td>59.135841</td>\n",
       "      <td>59.130615</td>\n",
       "      <td>60.118473</td>\n",
       "      <td>61.930611</td>\n",
       "      <td>62.509953</td>\n",
       "      <td>64.392212</td>\n",
       "      <td>66.825508</td>\n",
       "      <td>68.229317</td>\n",
       "      <td>73.133118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>25.597826</td>\n",
       "      <td>30.479658</td>\n",
       "      <td>32.840698</td>\n",
       "      <td>43.546482</td>\n",
       "      <td>48.089287</td>\n",
       "      <td>52.230488</td>\n",
       "      <td>53.142246</td>\n",
       "      <td>60.906494</td>\n",
       "      <td>61.734241</td>\n",
       "      <td>70.894989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>22.607437</td>\n",
       "      <td>30.184090</td>\n",
       "      <td>30.920879</td>\n",
       "      <td>41.313629</td>\n",
       "      <td>47.218407</td>\n",
       "      <td>51.852074</td>\n",
       "      <td>53.119232</td>\n",
       "      <td>59.029484</td>\n",
       "      <td>60.970016</td>\n",
       "      <td>71.511536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>17.353441</td>\n",
       "      <td>28.799351</td>\n",
       "      <td>30.064390</td>\n",
       "      <td>40.477009</td>\n",
       "      <td>47.875137</td>\n",
       "      <td>52.001602</td>\n",
       "      <td>57.249516</td>\n",
       "      <td>59.008663</td>\n",
       "      <td>63.138569</td>\n",
       "      <td>71.607170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>13.531326</td>\n",
       "      <td>23.367928</td>\n",
       "      <td>26.631954</td>\n",
       "      <td>39.987328</td>\n",
       "      <td>47.327034</td>\n",
       "      <td>53.208187</td>\n",
       "      <td>57.857674</td>\n",
       "      <td>59.643486</td>\n",
       "      <td>63.543201</td>\n",
       "      <td>71.594788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>14.078892</td>\n",
       "      <td>22.923355</td>\n",
       "      <td>25.719780</td>\n",
       "      <td>36.976070</td>\n",
       "      <td>45.137184</td>\n",
       "      <td>50.250908</td>\n",
       "      <td>52.601181</td>\n",
       "      <td>56.738323</td>\n",
       "      <td>62.051434</td>\n",
       "      <td>69.199371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>16.519575</td>\n",
       "      <td>27.044680</td>\n",
       "      <td>29.598755</td>\n",
       "      <td>33.275780</td>\n",
       "      <td>42.567791</td>\n",
       "      <td>49.462658</td>\n",
       "      <td>52.309090</td>\n",
       "      <td>59.266098</td>\n",
       "      <td>60.662495</td>\n",
       "      <td>68.161102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>17.595810</td>\n",
       "      <td>26.452866</td>\n",
       "      <td>29.041847</td>\n",
       "      <td>32.323463</td>\n",
       "      <td>33.854130</td>\n",
       "      <td>45.722637</td>\n",
       "      <td>55.447598</td>\n",
       "      <td>57.981567</td>\n",
       "      <td>62.340557</td>\n",
       "      <td>69.398178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>15.872547</td>\n",
       "      <td>23.500225</td>\n",
       "      <td>26.330027</td>\n",
       "      <td>31.210203</td>\n",
       "      <td>32.405453</td>\n",
       "      <td>40.784595</td>\n",
       "      <td>56.308880</td>\n",
       "      <td>58.129292</td>\n",
       "      <td>63.886032</td>\n",
       "      <td>72.072075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.270657</td>\n",
       "      <td>21.473568</td>\n",
       "      <td>24.275621</td>\n",
       "      <td>29.468462</td>\n",
       "      <td>32.560013</td>\n",
       "      <td>37.765759</td>\n",
       "      <td>54.597122</td>\n",
       "      <td>58.777283</td>\n",
       "      <td>64.359390</td>\n",
       "      <td>74.496994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            -100       -85        -80         -70         -60         -50   \\\n",
       "10000  95.156166  97.072845  98.251839  101.292374  104.738571  107.212234   \n",
       "8000   80.992577  80.656555  81.296867   83.045113   85.806847   89.005676   \n",
       "7000   72.939873  73.574600  73.921783   74.942772   76.689171   79.570923   \n",
       "6000   56.404900  66.701172  66.736687   67.701447   68.986969   70.598473   \n",
       "5500   36.528988  63.695129  63.519688   64.266251   65.331619   66.565918   \n",
       "5000   29.380203  59.135841  59.130615   60.118473   61.930611   62.509953   \n",
       "4000   25.597826  30.479658  32.840698   43.546482   48.089287   52.230488   \n",
       "3500   22.607437  30.184090  30.920879   41.313629   47.218407   51.852074   \n",
       "3000   17.353441  28.799351  30.064390   40.477009   47.875137   52.001602   \n",
       "2500   13.531326  23.367928  26.631954   39.987328   47.327034   53.208187   \n",
       "2000   14.078892  22.923355  25.719780   36.976070   45.137184   50.250908   \n",
       "1500   16.519575  27.044680  29.598755   33.275780   42.567791   49.462658   \n",
       "1000   17.595810  26.452866  29.041847   32.323463   33.854130   45.722637   \n",
       "500    15.872547  23.500225  26.330027   31.210203   32.405453   40.784595   \n",
       "0      12.270657  21.473568  24.275621   29.468462   32.560013   37.765759   \n",
       "\n",
       "             -40         -30         -20          0    \n",
       "10000  109.492973  112.391235  114.940308  120.652802  \n",
       "8000    91.346283   94.262543   96.579269  102.227219  \n",
       "7000    82.304031   85.109131   87.276886   92.838837  \n",
       "6000    73.476135   75.995560   77.774910   82.798866  \n",
       "5500    68.902870   71.466042   73.106491   77.596542  \n",
       "5000    64.392212   66.825508   68.229317   73.133118  \n",
       "4000    53.142246   60.906494   61.734241   70.894989  \n",
       "3500    53.119232   59.029484   60.970016   71.511536  \n",
       "3000    57.249516   59.008663   63.138569   71.607170  \n",
       "2500    57.857674   59.643486   63.543201   71.594788  \n",
       "2000    52.601181   56.738323   62.051434   69.199371  \n",
       "1500    52.309090   59.266098   60.662495   68.161102  \n",
       "1000    55.447598   57.981567   62.340557   69.398178  \n",
       "500     56.308880   58.129292   63.886032   72.072075  \n",
       "0       54.597122   58.777283   64.359390   74.496994  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuel_rows=[]\n",
    "for rpm in fuel_rpms:\n",
    "    fuel_row = []\n",
    "    \n",
    "    for load in fuel_loads:    \n",
    "        fuel_row.append(predict_base_fuel(rpm, load))\n",
    "        \n",
    "    fuel_rows.append(fuel_row)\n",
    "    \n",
    "    fuel_map=np.array(fuel_rows)\n",
    "    \n",
    "df_base_fuel_map_predict=pd.DataFrame(fuel_map)\n",
    "df_base_fuel_map_predict.columns = fuel_loads\n",
    "df_base_fuel_map_predict.index = fuel_rpms\n",
    "df_base_fuel_map_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
